{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class Index</th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reuters)</td>\n",
       "      <td>Reuters - Short-sellers, Wall Street's dwindli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "      <td>Reuters - Private investment firm Carlyle Grou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class Index                                              Title  \\\n",
       "0            3  Wall St. Bears Claw Back Into the Black (Reuters)   \n",
       "1            3  Carlyle Looks Toward Commercial Aerospace (Reu...   \n",
       "\n",
       "                                         Description  \n",
       "0  Reuters - Short-sellers, Wall Street's dwindli...  \n",
       "1  Reuters - Private investment firm Carlyle Grou...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dxlab/jupyter/envs/sum_py37/lib/python3.7/site-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Number of ham and spam messages')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEWCAYAAACnlKo3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAcMElEQVR4nO3dfbQddX3v8feHBAQFBSWlQNBwFa1oFTVCWrwtxSsEWhvaq15olWjR2BZaXdeqYO8SRF21q1arVmlpiYBWkfpQcr20FBG1VnkISnnUkgtoggiR8OgDveD3/jG/yPR4TjiZZJ/NyXm/1trrzPx+8/Cb2efsz57fzJlJVSFJ0hDbjbsBkqTZyxCRJA1miEiSBjNEJEmDGSKSpMEMEUnSYIaItliSM5O8Y0zrTpIPJ7kzyWWT1L8yyZfH0batKckhSdaNux3SRIbINijJzUluT/KYXtmrk3xhjM0alRcALwIWVtWB426MNNcYItuuecDrxt2IzZVk3mbO8iTg5qr6/ijaI2nTDJFt158Bf5Rk14kVSRYlqSTze2VfSPLqNvzKJP+a5L1J7kpyY5JfbOVr21HO8gmL3T3JhUnuTfLFJE/qLfvnWt2GJN9M8rJe3ZlJTktyfpLvA78ySXv3SrKqzb8myWta+XHA3wK/kOS+JG+bamckeXfr8ropyRG98lclub61+8Ykr+3VHZJkXZI3tW2+NclRSY5M8u+tPW/ZxDp/NcnXk9zT9tspk7wHy5N8O8n3kvxxr36ntm/uTHId8PxNrCftvbq9revqJM/s7d+/2sR7877WtnuSXJHkv/bqTkny90k+2ua9OslTk5zU1rU2yWGbaNfNSd6Y5Kok309yRpI9kvxjW97nkuzWm35Jkq+037l/S3JIr+6V7f25t72Hv93Kn9K26e62Dz8xzW3bKclZbf9e397jdb36vZJ8Ksn6tr4/7NUdmGR1W+5tSd4z1T6YE6rK1zb2Am4G/hvwaeAdrezVwBfa8CKggPm9eb4AvLoNvxJ4AHgV3RHNO4BvAx8EHgUcBtwL7NymP7ON/1Krfx/w5Vb3GGBtW9Z84DnA94D9e/PeDRxM96Vmx0m250vAh4AdgQOA9cChvbZ+eRP74pXA/wNe07bl94DvAGn1vwo8GQjwy8APgOe2ukPafngrsH1bxnrgY8AuwDOAHwL7TrHuQ4Cfb9v1LOA24KgJ78HfADsBzwbuB57e6t8F/AvweGAf4Bpg3RTrORy4Ati1bcfTgT0f7r1p9S8HntDemzcA3934HgCnAD9qy58PnA3cBPxxb3/c9DC/h5cAewB7A7cDX2u/AzsCnwdObtPuDdwBHNn214va+AK636F7gKe1afcEntGGP97as11b5gumuW3vAr4I7AYsBK7auH/bsq5o7/sOwH8BbgQOb/VfBV7RhncGloz7b36snzfjboCvEbypD4XIM+k+oBew+SFyQ6/u59v0e/TK7gAOaMNnAuf06nYGHqT78PsfwL9MaN9f9z48zgTO3sS27NOWtUuv7E+AM3ttfbgQWdMbf3Tblp+dYvp/AF7Xhg+hC4l5bXyXNu9BvemvoAXDNN6XvwDeO+E9WNirvww4ug3fCCzt1a1g6hA5FPh3YAmw3YS6Kd+bKZZ1J/DsNnwKcGGv7sXAfZPsj1038Xv4273xTwGn9cb/APiHNvxm4CMT5r8AWE4XIncB/x3YacI0ZwOn9/fjJvZ/f9t+Egpt/NU8FCIHAd+eMO9JwIfb8JeAtwG7b8nf6bbysjtrG1ZV1wCfBU4cMPttveEftuVNLNu5N762t977gA3AXnTnLA5qXRR3JbkL+G3gZyebdxJ7ARuq6t5e2bfovrlO13d7bftBG9wZIMkRSS5pXVN30X0T3r037x1V9WAb/mH7uan98BNJDkpycesSuRv43QnL/k9tozsK2risvfjP++VbU21cVX0e+Eu6I8Xbk5ye5LG9SaZ6b0jyR6075+62/Y+b0MaJ2/q9SfbHpNs/xfxT7bsnAS+d8HvyArojqu/TfRn5XeDWJP8nyc+1+d5Ed/R1WZJrk/zOxoU/zLZN3L/94ScBe01oy1vojqgAjgOeCnwjyeVJfm0T27/NM0S2fSfTdTv0P3Q3noR+dK+s/6E+xD4bB5LsTNcN8x26P84vVtWuvdfOVfV7vXk3dSvp7wCPT7JLr+yJwC1b2F6SPIru2/G76Y6ydgXOp/tQ2ho+Bqyi+9b/OOCvNmPZt9Lbp3TbPKWqen9VPQ/Yn+4D7o296knfm3aO4E3Ay4Dd2vbfvRlt3JrW0h2J9H9PHlNV7wKoqguq6kV0XVnfoOsGpKq+W1Wvqaq9gNcCH2rnSR5u226l68baqL+v19J10/XbsktVHdnWeUNVHQP8DPCnwCfTuxJyrjFEtnFVtQb4BPCHvbL1dB/CL08yr317e/IWrurIJC9IsgPwduCSqlpLdyT01CSvSLJ9ez0/ydOn2f61wFeAP0myY5Jn0X0T/OgWthe6/u5H0Z3neCDdCfcpTxQPsAvdUdSPkhwI/NZmzHsucFKS3ZIspOv6mVTbnwcl2Z7uC8KPgB/3JpnqvdmF7pzPemB+krcCj2U8Pgq8OMnh7Xdyx3QXNixsJ+OXtQ/q++m61H4MkOSlbf9A111Vre7htq2/f/cGTujVXQbcm+TN7QT8vCTPTPL8ts6XJ1lQVT+m62aD/7y/5xRDZG44la5fue81dN9W76A7QfyVLVzHx+iOejYAz6M7qUnrhjoMOJruqOK7dN/eHrUZyz6G7hzCd4DP0J1P+dwWtndj2/6Q7gPlTroP+VVbutye3wdOTXIv3Unaczdj3rfRdWHdBPwz8JFNTPtYum/md7Z57qC7Om+jSd8bunMO/0R3PuVbdOGzqa7FkWmhtoyu22h9a8cb6T6jtgP+J937v4HuAoiNR7LPBy5Nch/de/e6qrqRh9+2U4F1dPv3c8An6QKK1l33a3QXcdxEdyHI39J1hwEsBa5t63wf3XmsHzJHbbxCRdI2KMmZdCeM/9e42/JIluT36MLgl8fdltnGIxFJc06SPZMcnGS7JE+juwT4M+Nu12w0/+EnkaRtzg50l5rvS3de4xy6/0XSZrI7S5I0mN1ZkqTB5lx31u67716LFi0adzMkaVa54oorvldVCyaWz7kQWbRoEatXrx53MyRpVkky6V0T7M6SJA1miEiSBjNEJEmDGSKSpMEMEUnSYIaIJGmwkYVIu5XzZemelXxt2vOvk+yb5NJ0z8r+RLs9NUke1cbXtPpFvWWd1Mq/meTwXvnSVrYmyZAHL0mStsAoj0Tup3sO9rPpbqm8NMkSutuAv7eqnkJ36+rj2vTHAXe28ve26UiyP91txJ9BdwvmD7X7+8+je5LbEXQP4jmmTStJmiEjC5Hq3NdGt2+vonse9Cdb+VnAUW14WRun1b8wSVr5OVV1f1XdBKwBDmyvNVV1Y1X9B90N1JaNanskST9tpP+x3o4WrgCeQnfU8H+Bu6rqgTbJOh56bOvetIfGVNUD7ZnUT2jll/QW259n7YTyg6ZoxwpgBcATn7jJp4zyvDeePY0tmxuu+LNjt3gZ3z7157dCS7YNT3zr1Vs0/8EfOHgrtWT2+9c/+NctXsYXf8lHh2z0y1/64uB5R3pivaoerKoD6J5lfCDwc6Nc3ybacXpVLa6qxQsW/NStXyRJA83I1VlVdRdwMfALwK5JNh4BLaR71jft5z4Arf5xdI/5/En5hHmmKpckzZBRXp21IMmubXgn4EXA9XRh8pI22XLgvDa8qo3T6j9f3cNOVgFHt6u39gX2Ay4DLgf2a1d77UB38n1rPh9bkvQwRnlOZE/grHZeZDvg3Kr6bJLrgHOSvAP4OnBGm/4M4CNJ1gAb6EKBqro2ybnAdcADwPFV9SBAkhOAC4B5wMqqunaE2yNJmmBkIVJVVwHPmaT8RrrzIxPLfwS8dIplvRN45yTl5wPnb3FjJUmD+B/rkqTBDBFJ0mCGiCRpMENEkjSYISJJGswQkSQNZohIkgYzRCRJgxkikqTBDBFJ0mCGiCRpMENEkjSYISJJGswQkSQNZohIkgYzRCRJgxkikqTBDBFJ0mCGiCRpMENEkjSYISJJGswQkSQNZohIkgYzRCRJgxkikqTBRhYiSfZJcnGS65Jcm+R1rfyUJLckubK9juzNc1KSNUm+meTwXvnSVrYmyYm98n2TXNrKP5Fkh1FtjyTpp43ySOQB4A1VtT+wBDg+yf6t7r1VdUB7nQ/Q6o4GngEsBT6UZF6SecAHgSOA/YFjesv507aspwB3AseNcHskSROMLESq6taq+lobvhe4Hth7E7MsA86pqvur6iZgDXBge62pqhur6j+Ac4BlSQIcCnyyzX8WcNRINkaSNKkZOSeSZBHwHODSVnRCkquSrEyyWyvbG1jbm21dK5uq/AnAXVX1wITyyda/IsnqJKvXr1+/NTZJksQMhEiSnYFPAa+vqnuA04AnAwcAtwJ/Puo2VNXpVbW4qhYvWLBg1KuTpDlj/igXnmR7ugD5u6r6NEBV3dar/xvgs230FmCf3uwLWxlTlN8B7Jpkfjsa6U8vSZoBo7w6K8AZwPVV9Z5e+Z69yX4DuKYNrwKOTvKoJPsC+wGXAZcD+7UrsXagO/m+qqoKuBh4SZt/OXDeqLZHkvTTRnkkcjDwCuDqJFe2srfQXV11AFDAzcBrAarq2iTnAtfRXdl1fFU9CJDkBOACYB6wsqqubct7M3BOkncAX6cLLUnSDBlZiFTVl4FMUnX+JuZ5J/DOScrPn2y+qrqR7uotSdIY+B/rkqTBDBFJ0mCGiCRpMENEkjSYISJJGswQkSQNZohIkgYzRCRJgxkikqTBDBFJ0mCGiCRpMENEkjSYISJJGswQkSQNZohIkgYzRCRJgxkikqTBDBFJ0mCGiCRpMENEkjSYISJJGswQkSQNZohIkgYzRCRJgxkikqTBDBFJ0mAjC5Ek+yS5OMl1Sa5N8rpW/vgkFya5of3crZUnyfuTrElyVZLn9pa1vE1/Q5LlvfLnJbm6zfP+JBnV9kiSftooj0QeAN5QVfsDS4Djk+wPnAhcVFX7ARe1cYAjgP3aawVwGnShA5wMHAQcCJy8MXjaNK/pzbd0hNsjSZpgZCFSVbdW1dfa8L3A9cDewDLgrDbZWcBRbXgZcHZ1LgF2TbIncDhwYVVtqKo7gQuBpa3usVV1SVUVcHZvWZKkGTAj50SSLAKeA1wK7FFVt7aq7wJ7tOG9gbW92da1sk2Vr5ukfLL1r0iyOsnq9evXb9nGSJJ+YuQhkmRn4FPA66vqnn5dO4KoUbehqk6vqsVVtXjBggWjXp0kzRkjDZEk29MFyN9V1adb8W2tK4r28/ZWfguwT2/2ha1sU+ULJymXJM2QUV6dFeAM4Pqqek+vahWw8Qqr5cB5vfJj21VaS4C7W7fXBcBhSXZrJ9QPAy5odfckWdLWdWxvWZKkGTB/hMs+GHgFcHWSK1vZW4B3AecmOQ74FvCyVnc+cCSwBvgB8CqAqtqQ5O3A5W26U6tqQxv+feBMYCfgH9tLkjRDRhYiVfVlYKr/23jhJNMXcPwUy1oJrJykfDXwzC1opiRpC/gf65KkwQwRSdJghogkaTBDRJI0mCEiSRrMEJEkDWaISJIGM0QkSYMZIpKkwQwRSdJghogkaTBDRJI02LRCJMlF0ymTJM0tm7yLb5IdgUcDu7dneWy8K+9jmeJRtJKkuePhbgX/WuD1wF7AFTwUIvcAfzm6ZkmSZoNNhkhVvQ94X5I/qKoPzFCbJEmzxLQeSlVVH0jyi8Ci/jxVdfaI2iVJmgWmFSJJPgI8GbgSeLAVF2CISNIcNt3H4y4G9m+PsJUkCZj+/4lcA/zsKBsiSZp9pnsksjtwXZLLgPs3FlbVr4+kVZKkWWG6IXLKKBshSZqdpnt11hdH3RBJ0uwz3auz7qW7GgtgB2B74PtV9dhRNUyS9Mg33SORXTYOJwmwDFgyqkZJkmaHzb6Lb3X+ATh8U9MlWZnk9iTX9MpOSXJLkivb68he3UlJ1iT5ZpLDe+VLW9maJCf2yvdNcmkr/0SSHTZ3WyRJW2a63Vm/2Rvdju7/Rn70MLOdSXd/rYn/kPjeqnr3hOXvDxwNPIPuPl2fS/LUVv1B4EXAOuDyJKuq6jrgT9uyzknyV8BxwGnT2R5J0tYx3auzXtwbfgC4ma5La0pV9aUki6a5/GXAOVV1P3BTkjXAga1uTVXdCJDkHGBZkuuBQ4HfatOcRXcFmSEiSTNouudEXrUV13lCkmOB1cAbqupOutvKX9KbZh0P3Wp+7YTyg4AnAHdV1QOTTP9TkqwAVgA88YlP3BrbIEli+g+lWpjkM+0cx+1JPpVk4YD1nUZ3D64DgFuBPx+wjM1WVadX1eKqWrxgwYKZWKUkzQnTPbH+YWAV3fmKvYD/3co2S1XdVlUPVtWPgb/hoS6rW4B9epMubGVTld8B7Jpk/oRySdIMmm6ILKiqD1fVA+11JrDZX+mT7Nkb/Q26e3JBF1BHJ3lUkn2B/YDLgMuB/dqVWDvQnXxf1W4EeTHwkjb/cuC8zW2PJGnLTPfE+h1JXg58vI0fQ3c0MKUkHwcOoXu07jrgZOCQJAfQ/ePizXRPTqSqrk1yLnAd3Yn746vqwbacE4ALgHnAyqq6tq3izcA5Sd4BfB04Y5rbIknaSqYbIr8DfAB4L10AfAV45aZmqKpjJime8oO+qt4JvHOS8vOB8ycpv5GHusMkSWMw3RA5FVjerqQiyeOBd9OFiyRpjpruOZFnbQwQgKraADxnNE2SJM0W0w2R7ZLstnGkHYlM9yhGkrSNmm4Q/Dnw1SR/38ZfyiTnLyRJc8t0/2P97CSr6W41AvCb7f5VkqQ5bNpdUi00DA5J0k9s9q3gJUnayBCRJA1miEiSBjNEJEmDGSKSpMEMEUnSYIaIJGkwQ0SSNJghIkkazBCRJA1miEiSBjNEJEmDGSKSpMEMEUnSYIaIJGkwQ0SSNJghIkkazBCRJA1miEiSBjNEJEmDjSxEkqxMcnuSa3plj09yYZIb2s/dWnmSvD/JmiRXJXlub57lbfobkizvlT8vydVtnvcnyai2RZI0uVEeiZwJLJ1QdiJwUVXtB1zUxgGOAPZrrxXAadCFDnAycBBwIHDyxuBp07ymN9/EdUmSRmxkIVJVXwI2TCheBpzVhs8CjuqVn12dS4Bdk+wJHA5cWFUbqupO4EJgaat7bFVdUlUFnN1bliRphsz0OZE9qurWNvxdYI82vDewtjfdula2qfJ1k5RPKsmKJKuTrF6/fv2WbYEk6SfGdmK9HUHUDK3r9KpaXFWLFyxYMBOrlKQ5YaZD5LbWFUX7eXsrvwXYpzfdwla2qfKFk5RLkmbQTIfIKmDjFVbLgfN65ce2q7SWAHe3bq8LgMOS7NZOqB8GXNDq7kmypF2VdWxvWZKkGTJ/VAtO8nHgEGD3JOvorrJ6F3BukuOAbwEva5OfDxwJrAF+ALwKoKo2JHk7cHmb7tSq2niy/vfprgDbCfjH9pIkzaCRhUhVHTNF1QsnmbaA46dYzkpg5STlq4FnbkkbJUlbxv9YlyQNZohIkgYzRCRJgxkikqTBDBFJ0mCGiCRpMENEkjSYISJJGswQkSQNZohIkgYzRCRJgxkikqTBDBFJ0mCGiCRpMENEkjSYISJJGswQkSQNZohIkgYzRCRJgxkikqTBDBFJ0mCGiCRpMENEkjSYISJJGswQkSQNNpYQSXJzkquTXJlkdSt7fJILk9zQfu7WypPk/UnWJLkqyXN7y1nepr8hyfJxbIskzWXjPBL5lao6oKoWt/ETgYuqaj/gojYOcASwX3utAE6DLnSAk4GDgAOBkzcGjyRpZjySurOWAWe14bOAo3rlZ1fnEmDXJHsChwMXVtWGqroTuBBYOsNtlqQ5bVwhUsA/J7kiyYpWtkdV3dqGvwvs0Yb3Btb25l3XyqYq/ylJViRZnWT1+vXrt9Y2SNKcN39M631BVd2S5GeAC5N8o19ZVZWkttbKqup04HSAxYsXb7XlStJcN5Yjkaq6pf28HfgM3TmN21o3Fe3n7W3yW4B9erMvbGVTlUuSZsiMh0iSxyTZZeMwcBhwDbAK2HiF1XLgvDa8Cji2XaW1BLi7dXtdAByWZLd2Qv2wViZJmiHj6M7aA/hMko3r/1hV/VOSy4FzkxwHfAt4WZv+fOBIYA3wA+BVAFW1IcnbgcvbdKdW1YaZ2wxJ0oyHSFXdCDx7kvI7gBdOUl7A8VMsayWwcmu3UZI0PY+kS3wlSbOMISJJGswQkSQNZohIkgYzRCRJgxkikqTBDBFJ0mCGiCRpMENEkjSYISJJGswQkSQNZohIkgYzRCRJgxkikqTBDBFJ0mCGiCRpMENEkjSYISJJGswQkSQNZohIkgYzRCRJgxkikqTBDBFJ0mCGiCRpMENEkjSYISJJGmzWh0iSpUm+mWRNkhPH3R5JmktmdYgkmQd8EDgC2B84Jsn+422VJM0dszpEgAOBNVV1Y1X9B3AOsGzMbZKkOSNVNe42DJbkJcDSqnp1G38FcFBVnTBhuhXAijb6NOCbM9rQYXYHvjfuRmwj3Jdbl/tz65ot+/NJVbVgYuH8cbRkplXV6cDp427H5kiyuqoWj7sd2wL35dbl/ty6Zvv+nO3dWbcA+/TGF7YySdIMmO0hcjmwX5J9k+wAHA2sGnObJGnOmNXdWVX1QJITgAuAecDKqrp2zM3aWmZV99sjnPty63J/bl2zen/O6hPrkqTxmu3dWZKkMTJEJEmDGSKPIElWJrk9yTXjbsu2IMk+SS5Ocl2Sa5O8btxtms2S7JjksiT/1vbn28bdptkuybwkX0/y2XG3ZShD5JHlTGDpuBuxDXkAeENV7Q8sAY73tjhb5H7g0Kp6NnAAsDTJkvE2adZ7HXD9uBuxJQyRR5Cq+hKwYdzt2FZU1a1V9bU2fC/dH+ve423V7FWd+9ro9u3llTkDJVkI/Crwt+Nuy5YwRDQnJFkEPAe4dMxNmdVa98uVwO3AhVXl/hzuL4A3AT8eczu2iCGibV6SnYFPAa+vqnvG3Z7ZrKoerKoD6O4OcWCSZ465SbNSkl8Dbq+qK8bdli1liGiblmR7ugD5u6r69Ljbs62oqruAi/Ec3lAHA7+e5Ga6u48fmuSj423SMIaItllJApwBXF9V7xl3e2a7JAuS7NqGdwJeBHxjrI2aparqpKpaWFWL6G7X9PmqevmYmzWIIfIIkuTjwFeBpyVZl+S4cbdpljsYeAXdt7wr2+vIcTdqFtsTuDjJVXT3rbuwqmbtpanaOrztiSRpMI9EJEmDGSKSpMEMEUnSYIaIJGkwQ0SSNJghIo1IkvsefqqfTHtKkj8a1fKlUTFEJEmDGSLSDEry4iSXtmdIfC7JHr3qZyf5apIbkrymN88bk1ye5Cqf4aFHGkNEmllfBpZU1XPo7pn0pl7ds4BDgV8A3ppkrySHAfsBB9I9w+N5SX5pZpssTW3+uBsgzTELgU8k2RPYAbipV3deVf0Q+GGSi+mC4wXAYcDX2zQ704XKl2auydLUDBFpZn0AeE9VrUpyCHBKr27iPYgKCPAnVfXXM9I6aTPZnSXNrMcBt7Th5RPqlrXnmD8BOITuJocXAL/TnolCkr2T/MxMNVZ6OB6JSKPz6CTreuPvoTvy+PskdwKfB/bt1V9F94yO3YG3V9V3gO8keTrw1e7O9twHvJzuyYLS2HkXX0nSYHZnSZIGM0QkSYMZIpKkwQwRSdJghogkaTBDRJI0mCEiSRrs/wPS+Q3R0GgQPgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(train['Class Index'])\n",
    "plt.xlabel('Label')\n",
    "plt.title('Number of ham and spam messages')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/dxlab/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text, stem=False):\n",
    "    text_cleaning_re = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
    "\n",
    "    text = re.sub(text_cleaning_re, ' ', str(text).lower()).strip()\n",
    "    tokens = []\n",
    "    for token in text.split():\n",
    "        if token not in stop_words:\n",
    "            if stem:\n",
    "                tokens.append(stemmer.stem(token))\n",
    "            else:\n",
    "                tokens.append(token)\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Description'] = train['Description'].apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Description'] = test['Description'].apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : 118341\n"
     ]
    }
   ],
   "source": [
    "train.drop_duplicates(subset = ['Description'], inplace=True)\n",
    "train = train.dropna(how='any')\n",
    "print('train :',len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test : 7591\n"
     ]
    }
   ],
   "source": [
    "test.drop_duplicates(subset = ['Description'], inplace=True)\n",
    "test = test.dropna(how='any')\n",
    "print('test :',len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 60000\n",
    "MAX_SEQUENCE_LENGTH = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size : 60514\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words = MAX_NB_WORDS, oov_token = '<OOV>')\n",
    "tokenizer.fit_on_texts(train['Description'])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"Vocabulary Size :\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<OOV>': 1,\n",
       " 'said': 2,\n",
       " 'new': 3,\n",
       " 'us': 4,\n",
       " 'reuters': 5,\n",
       " 'ap': 6,\n",
       " 'two': 7,\n",
       " 'first': 8,\n",
       " 'monday': 9,\n",
       " 'wednesday': 10}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(list(word_index.items())[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training X Shape: (118341, 50)\n",
      "Testing X Shape: (7591, 50)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "x_train = pad_sequences(tokenizer.texts_to_sequences(train['Description']),\n",
    "                        maxlen = MAX_SEQUENCE_LENGTH)\n",
    "x_test = pad_sequences(tokenizer.texts_to_sequences(test['Description']),\n",
    "                       maxlen = MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print(\"Training X Shape:\",x_train.shape)\n",
    "print(\"Testing X Shape:\",x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = train['Class Index'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train shape: (118341, 1)\n",
      "y_test shape: (7591, 1)\n"
     ]
    }
   ],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(train['Class Index'].to_list())\n",
    "\n",
    "y_train = encoder.transform(train['Class Index'].to_list())\n",
    "y_test = encoder.transform(test['Class Index'].to_list())\n",
    "\n",
    "y_train = y_train.reshape(-1,1)\n",
    "y_test = y_test.reshape(-1,1)\n",
    "\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, add, concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=100))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(13, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 300)          18000000  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 100, 300)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               160400    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 13)                1313      \n",
      "=================================================================\n",
      "Total params: 18,161,713\n",
      "Trainable params: 18,161,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 100) for input Tensor(\"embedding_1_input:0\", shape=(None, 100), dtype=float32), but it was called on an input with incompatible shape (None, 50).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 100) for input Tensor(\"embedding_1_input:0\", shape=(None, 100), dtype=float32), but it was called on an input with incompatible shape (None, 50).\n",
      "1578/1578 [==============================] - ETA: 0s - loss: 0.4108 - acc: 0.8530WARNING:tensorflow:Model was constructed with shape (None, 100) for input Tensor(\"embedding_1_input:0\", shape=(None, 100), dtype=float32), but it was called on an input with incompatible shape (None, 50).\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.90105, saving model to best_model.h5\n",
      "1578/1578 [==============================] - 302s 192ms/step - loss: 0.4108 - acc: 0.8530 - val_loss: 0.2934 - val_acc: 0.9011\n",
      "Epoch 2/10\n",
      "1578/1578 [==============================] - ETA: 0s - loss: 0.1888 - acc: 0.9370\n",
      "Epoch 00002: val_acc did not improve from 0.90105\n",
      "1578/1578 [==============================] - 249s 158ms/step - loss: 0.1888 - acc: 0.9370 - val_loss: 0.3033 - val_acc: 0.8961\n",
      "Epoch 3/10\n",
      "1578/1578 [==============================] - ETA: 0s - loss: 0.1212 - acc: 0.9591\n",
      "Epoch 00003: val_acc did not improve from 0.90105\n",
      "1578/1578 [==============================] - 202s 128ms/step - loss: 0.1212 - acc: 0.9591 - val_loss: 0.3461 - val_acc: 0.8887\n",
      "Epoch 4/10\n",
      "1578/1578 [==============================] - ETA: 0s - loss: 0.0785 - acc: 0.9736\n",
      "Epoch 00004: val_acc did not improve from 0.90105\n",
      "1578/1578 [==============================] - 202s 128ms/step - loss: 0.0785 - acc: 0.9736 - val_loss: 0.4315 - val_acc: 0.8830\n",
      "Epoch 5/10\n",
      "1578/1578 [==============================] - ETA: 0s - loss: 0.0526 - acc: 0.9814\n",
      "Epoch 00005: val_acc did not improve from 0.90105\n",
      "1578/1578 [==============================] - 201s 128ms/step - loss: 0.0526 - acc: 0.9814 - val_loss: 0.5064 - val_acc: 0.8742\n",
      "Epoch 00005: early stopping\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x_train, y_train, epochs=10, callbacks=[es, mc], batch_size=60, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238/238 [==============================] - 2s 8ms/step - loss: 0.3752 - acc: 0.8963\n",
      "Test set\n",
      "  Loss: 0.375\n",
      "  Accuracy: 0.896\n"
     ]
    }
   ],
   "source": [
    "accr = model.evaluate(x_test,y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class Index</th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reuters)</td>\n",
       "      <td>reuters shortsellers wall streets dwindlingban...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "      <td>reuters private investment firm carlyle groupw...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class Index                                              Title  \\\n",
       "0            3  Wall St. Bears Claw Back Into the Black (Reuters)   \n",
       "1            3  Carlyle Looks Toward Commercial Aerospace (Reu...   \n",
       "\n",
       "                                         Description  \n",
       "0  reuters shortsellers wall streets dwindlingban...  \n",
       "1  reuters private investment firm carlyle groupw...  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118341"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_t = []\n",
    "words_t = []\n",
    "\n",
    "train = train.reset_index()\n",
    "\n",
    "for i in range(len(train)):\n",
    "    sentences_t.append(str(train['Description'][i]))\n",
    "    words_t.append(sentences_t[i].split(' '))\n",
    "len(words_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x7f4c92273110>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 단어를 벡터화할 때 단어의 문맥적 의미를 보존\n",
    "\n",
    "# 파라메터값 지정\n",
    "num_features = 300 # 문자 벡터 차원 수\n",
    "min_word_count = 40 # 최소 문자 수 어휘의 크기를 의미있는 단어로 제한하는 데 도움이 된다. \n",
    "num_workers = 4 # 병렬 처리 스레드 수 CPU 쿼드코어\n",
    "context = 10 # 주변단어 앞뒤로 10개\n",
    "downsampling = 1e-3 # 문자 빈도 수 Downsample\n",
    "\n",
    "# 초기화 및 모델 학습\n",
    "from gensim.models import word2vec\n",
    "\n",
    "# 모델 학습\n",
    "model = word2vec.Word2Vec(words_t, \n",
    "                          workers=num_workers, \n",
    "                          size=num_features, \n",
    "                          min_count=min_word_count,\n",
    "                          window=context,\n",
    "                          sample=downsampling,\n",
    "                          sg=1)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec.load('word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size : 60513\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train['Description'])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"Vocabulary Size :\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training X Shape: (118341, 50)\n",
      "Testing X Shape: (7591, 50)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "x_train = pad_sequences(tokenizer.texts_to_sequences(train['Description']),\n",
    "                        maxlen = MAX_SEQUENCE_LENGTH)\n",
    "x_test = pad_sequences(tokenizer.texts_to_sequences(test['Description']),\n",
    "                       maxlen = MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print(\"Training X Shape:\",x_train.shape)\n",
    "print(\"Testing X Shape:\",x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dxlab/jupyter/envs/sum_py37/lib/python3.7/site-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/dxlab/jupyter/envs/sum_py37/lib/python3.7/site-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(60513, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "VOCAB_SIZE = len(tokenizer.index_word) + 1\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "\n",
    "# tokenizer에 있는 단어 사전을 순회하면서 word2vec의 300차원 vector를 가져옵니다\n",
    "for word, idx in tokenizer.word_index.items():\n",
    "    embedding_vector = w2v_model[word] if word in w2v_model else None\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[idx] = embedding_vector\n",
    "        \n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "model = Sequential()\n",
    "model.add(Embedding(VOCAB_SIZE,\n",
    "        EMBEDDING_DIM,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=MAX_SEQUENCE_LENGTH,\n",
    "        trainable=False))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(13, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 50, 300)           18153900  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d (SpatialDr (None, 50, 300)           0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               160400    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 13)                1313      \n",
      "=================================================================\n",
      "Total params: 18,315,613\n",
      "Trainable params: 161,713\n",
      "Non-trainable params: 18,153,900\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1578/1578 [==============================] - ETA: 0s - loss: 0.3679 - acc: 0.8744\n",
      "Epoch 00001: val_acc improved from -inf to 0.89222, saving model to best_model.h5\n",
      "1578/1578 [==============================] - 91s 58ms/step - loss: 0.3679 - acc: 0.8744 - val_loss: 0.2968 - val_acc: 0.8922\n",
      "Epoch 2/10\n",
      "1578/1578 [==============================] - ETA: 0s - loss: 0.2961 - acc: 0.8962\n",
      "Epoch 00002: val_acc improved from 0.89222 to 0.90435, saving model to best_model.h5\n",
      "1578/1578 [==============================] - 91s 57ms/step - loss: 0.2961 - acc: 0.8962 - val_loss: 0.2673 - val_acc: 0.9043\n",
      "Epoch 3/10\n",
      "1578/1578 [==============================] - ETA: 0s - loss: 0.2685 - acc: 0.9039\n",
      "Epoch 00003: val_acc did not improve from 0.90435\n",
      "1578/1578 [==============================] - 90s 57ms/step - loss: 0.2685 - acc: 0.9039 - val_loss: 0.2804 - val_acc: 0.8950\n",
      "Epoch 4/10\n",
      "1578/1578 [==============================] - ETA: 0s - loss: 0.2503 - acc: 0.9102\n",
      "Epoch 00004: val_acc improved from 0.90435 to 0.91161, saving model to best_model.h5\n",
      "1578/1578 [==============================] - 91s 57ms/step - loss: 0.2503 - acc: 0.9102 - val_loss: 0.2490 - val_acc: 0.9116\n",
      "Epoch 5/10\n",
      "1578/1578 [==============================] - ETA: 0s - loss: 0.2371 - acc: 0.9140\n",
      "Epoch 00005: val_acc did not improve from 0.91161\n",
      "1578/1578 [==============================] - 91s 57ms/step - loss: 0.2371 - acc: 0.9140 - val_loss: 0.2495 - val_acc: 0.9104\n",
      "Epoch 6/10\n",
      "1578/1578 [==============================] - ETA: 0s - loss: 0.2254 - acc: 0.9186\n",
      "Epoch 00006: val_acc did not improve from 0.91161\n",
      "1578/1578 [==============================] - 91s 57ms/step - loss: 0.2254 - acc: 0.9186 - val_loss: 0.2497 - val_acc: 0.9114\n",
      "Epoch 7/10\n",
      "1578/1578 [==============================] - ETA: 0s - loss: 0.2147 - acc: 0.9232\n",
      "Epoch 00007: val_acc improved from 0.91161 to 0.91225, saving model to best_model.h5\n",
      "1578/1578 [==============================] - 91s 58ms/step - loss: 0.2147 - acc: 0.9232 - val_loss: 0.2458 - val_acc: 0.9122\n",
      "Epoch 8/10\n",
      "1578/1578 [==============================] - ETA: 0s - loss: 0.2050 - acc: 0.9257\n",
      "Epoch 00008: val_acc improved from 0.91225 to 0.91237, saving model to best_model.h5\n",
      "1578/1578 [==============================] - 78s 49ms/step - loss: 0.2050 - acc: 0.9257 - val_loss: 0.2514 - val_acc: 0.9124\n",
      "Epoch 9/10\n",
      "1577/1578 [============================>.] - ETA: 0s - loss: 0.1969 - acc: 0.9282\n",
      "Epoch 00009: val_acc did not improve from 0.91237\n",
      "1578/1578 [==============================] - 62s 39ms/step - loss: 0.1969 - acc: 0.9282 - val_loss: 0.2511 - val_acc: 0.9123\n",
      "Epoch 10/10\n",
      "1577/1578 [============================>.] - ETA: 0s - loss: 0.1902 - acc: 0.9314\n",
      "Epoch 00010: val_acc improved from 0.91237 to 0.91314, saving model to best_model.h5\n",
      "1578/1578 [==============================] - 62s 39ms/step - loss: 0.1902 - acc: 0.9314 - val_loss: 0.2525 - val_acc: 0.9131\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x_train, y_train, epochs=10, callbacks=[es, mc], batch_size=60, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238/238 [==============================] - 2s 8ms/step - loss: 0.2443 - acc: 0.9169\n",
      "Test set\n",
      "  Loss: 0.244\n",
      "  Accuracy: 0.917\n"
     ]
    }
   ],
   "source": [
    "accr = model.evaluate(x_test,y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "ft_model = FastText(words_t, size=300, window=5, min_count=5, workers=4, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dxlab/jupyter/envs/sum_py37/lib/python3.7/site-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/dxlab/jupyter/envs/sum_py37/lib/python3.7/site-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(60514, 300)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "VOCAB_SIZE = len(tokenizer.index_word) + 1\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "\n",
    "# tokenizer에 있는 단어 사전을 순회하면서 word2vec의 300차원 vector를 가져옵니다\n",
    "for word, idx in tokenizer.word_index.items():\n",
    "    embedding_vector = ft_model[word] if word in ft_model else None\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[idx] = embedding_vector\n",
    "        \n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "model = Sequential()\n",
    "model.add(Embedding(VOCAB_SIZE,\n",
    "        EMBEDDING_DIM,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=MAX_SEQUENCE_LENGTH,\n",
    "        trainable=False))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(13, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 50, 300)           18154200  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_2 (Spatial (None, 50, 300)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               160400    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 13)                1313      \n",
      "=================================================================\n",
      "Total params: 18,315,913\n",
      "Trainable params: 161,713\n",
      "Non-trainable params: 18,154,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1577/1578 [============================>.] - ETA: 0s - loss: 0.3712 - acc: 0.8729\n",
      "Epoch 00001: val_acc did not improve from 0.90105\n",
      "1578/1578 [==============================] - 61s 39ms/step - loss: 0.3713 - acc: 0.8729 - val_loss: 0.2842 - val_acc: 0.8964\n",
      "Epoch 2/10\n",
      "1577/1578 [============================>.] - ETA: 0s - loss: 0.2949 - acc: 0.8961\n",
      "Epoch 00002: val_acc improved from 0.90105 to 0.90219, saving model to best_model.h5\n",
      "1578/1578 [==============================] - 62s 39ms/step - loss: 0.2949 - acc: 0.8961 - val_loss: 0.2675 - val_acc: 0.9022\n",
      "Epoch 3/10\n",
      "1577/1578 [============================>.] - ETA: 0s - loss: 0.2684 - acc: 0.9047\n",
      "Epoch 00003: val_acc improved from 0.90219 to 0.90595, saving model to best_model.h5\n",
      "1578/1578 [==============================] - 61s 39ms/step - loss: 0.2684 - acc: 0.9047 - val_loss: 0.2554 - val_acc: 0.9060\n",
      "Epoch 4/10\n",
      "1577/1578 [============================>.] - ETA: 0s - loss: 0.2488 - acc: 0.9110\n",
      "Epoch 00004: val_acc improved from 0.90595 to 0.90811, saving model to best_model.h5\n",
      "1578/1578 [==============================] - 62s 39ms/step - loss: 0.2487 - acc: 0.9110 - val_loss: 0.2525 - val_acc: 0.9081\n",
      "Epoch 5/10\n",
      "1577/1578 [============================>.] - ETA: 0s - loss: 0.2365 - acc: 0.9162\n",
      "Epoch 00005: val_acc improved from 0.90811 to 0.90819, saving model to best_model.h5\n",
      "1578/1578 [==============================] - 61s 39ms/step - loss: 0.2365 - acc: 0.9162 - val_loss: 0.2506 - val_acc: 0.9082\n",
      "Epoch 6/10\n",
      "1577/1578 [============================>.] - ETA: 0s - loss: 0.2240 - acc: 0.9185\n",
      "Epoch 00006: val_acc improved from 0.90819 to 0.91166, saving model to best_model.h5\n",
      "1578/1578 [==============================] - 61s 39ms/step - loss: 0.2240 - acc: 0.9185 - val_loss: 0.2450 - val_acc: 0.9117\n",
      "Epoch 7/10\n",
      "1577/1578 [============================>.] - ETA: 0s - loss: 0.2150 - acc: 0.9218\n",
      "Epoch 00007: val_acc did not improve from 0.91166\n",
      "1578/1578 [==============================] - 61s 39ms/step - loss: 0.2150 - acc: 0.9218 - val_loss: 0.2469 - val_acc: 0.9109\n",
      "Epoch 8/10\n",
      "1577/1578 [============================>.] - ETA: 0s - loss: 0.2053 - acc: 0.9257\n",
      "Epoch 00008: val_acc did not improve from 0.91166\n",
      "1578/1578 [==============================] - 61s 39ms/step - loss: 0.2054 - acc: 0.9257 - val_loss: 0.2513 - val_acc: 0.9107\n",
      "Epoch 9/10\n",
      "1577/1578 [============================>.] - ETA: 0s - loss: 0.1973 - acc: 0.9292\n",
      "Epoch 00009: val_acc improved from 0.91166 to 0.91237, saving model to best_model.h5\n",
      "1578/1578 [==============================] - 61s 39ms/step - loss: 0.1974 - acc: 0.9291 - val_loss: 0.2457 - val_acc: 0.9124\n",
      "Epoch 10/10\n",
      "1577/1578 [============================>.] - ETA: 0s - loss: 0.1900 - acc: 0.9312\n",
      "Epoch 00010: val_acc did not improve from 0.91237\n",
      "1578/1578 [==============================] - 61s 39ms/step - loss: 0.1900 - acc: 0.9312 - val_loss: 0.2512 - val_acc: 0.9118\n",
      "Epoch 00010: early stopping\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x_train, y_train, epochs=10, callbacks=[es, mc], batch_size=60, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238/238 [==============================] - 1s 5ms/step - loss: 0.2431 - acc: 0.9161\n",
      "Test set\n",
      "  Loss: 0.243\n",
      "  Accuracy: 0.916\n"
     ]
    }
   ],
   "source": [
    "accr = model.evaluate(x_test,y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sum_py37",
   "language": "python",
   "name": "sum_py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
